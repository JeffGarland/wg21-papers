\section{Parallel Algorithms}
\lstset{escapeinside={/*!}{*/}}

\subsection{Example}
Consider the example in \lst{simd foreach}.
\begin{lstlisting}[style=Vc,numbers=left,float,label=lst:simd foreach,caption={
  Example using \simdEP with \code{iota} and \code{for_each}.
}]
std::vector<float> data;
data.resize(99);
iota(/*!\simdEP{}*/, data.begin(), data.end(), 0.f);
for_each(/*!\simdEP{}*/, data.begin(), data.end(), [](auto &x) {
  x *= x;
});
\end{lstlisting}
The \code{iota} and \code{for_each} functions each could create an internal \simd iterator adaptor, depending on the iterator category.
Being able to determine whether the storage, the iterator points to, is contiguous, is most important in this context as it enables vector loads and stores.
Since the \std\type{vector} iterators are \emph{contiguous iterators}, the example implementations shown in \lst{simd iota implementation} and \lst{simd foreach implementation} could be used for the example.
\lstinputlisting[style=Vc,numbers=left,float,label=lst:simd iota implementation,caption={
  Implementation idea for the \code{iota} function used in \lst{simd foreach}.
}]{iota.cpp}
\lstinputlisting[style=Vc,numbers=left,float,label=lst:simd foreach implementation,caption={
  Implementation idea for the \code{for_each} function used in \lst{simd foreach}.
}]{foreach.cpp}

Both implementations might be improved with a prologue that enables aligned loads and stores.
Also note that \code{for_each} allows the \code{Function} parameter to mutate the argument if the iterator is a mutable iterator.
The implementation uses a compile-time trait to determine whether the function \code{f} uses a reference parameter, in which case it stores the temporary \simd object back.
Otherwise, the store is optimized away.

\fig{simd iota} shows a visualization how the \code{iota} implementation works.
The \code{init} \simd object is stored via vector stores to 4 (assuming native \simd[::size() == 4]) elements in the \std\type{vector}.
In each iteration the \code{init} object is incremented by \simd[::size()] and stored to the following elements in the \std\type{vector}.
Since the \std\type{vector} has 99 elements, the last three elements cannot be initialized with a vector store of four elements.
Instead the \code{epilogue} recursion generates a new \code{init} \simd object for size 2 and subsequently for size 1.

\fig{simd foreach} visualizes the end of the \code{for_each} implementation.
The main \code{for} loop processes four elements of the \std\type{vector} in parallel.
It executes a vector load, calls the user-provided function with the temporary \simd object, and executes a vector store back to the same memory location.
The remaining three elements are again handled by an \code{epilogue} recursion which divides the number of processed elements by 2 with every step.

For both algorithms it would be perfectly valid to implement the epilogue as a sequential loop using \simd objects with size 1.

\begin{figure}[]
  \centering
  \begin{tikzpicture}
    \ttfamily
    \vInit
    \vMemoryNode{6}
    \vMemoryMark[0.25]{2}{0,1,2,3,4,5,6,7,8,9,10,11}
    \vTranslate{-2,10}
    \draw[fill=black] (p) circle (.2em) ++(0,\vNodeHeight) circle (.2em) ++(0,\vNodeHeight) circle (.2em);
    \vMemoryNode{5}
    \vMemoryMark[0.25]{2}{0,1,2,3,4,5,6}
    \vTranslate{-6,-16}
    \vNode{0,1,2,3}
    \vArrowsStraight{A0/Mark0,A1/Mark1,A2/Mark2,A3/Mark3}
    \vTranslate{-6,4.5}
    \vNode{0,1,2,3}
    \vOperations{+}
    \vNode{4,4,4,4}
    \vOperations{=}
    \vNode{4,5,6,7}
    \vArrowsStraight{D0/Mark4,D1/Mark5,D2/Mark6,D3/Mark7}
    \vTranslate{-6,4.5}
    \vNode{4,5,6,7}
    \vOperations{+}
    \vNode{4,4,4,4}
    \vOperations{=}
    \vNode{8,9,10,11}
    \vArrowsStraight{G0/Mark8,G1/Mark9,G2/Mark10,G3/Mark11}
    \vTranslate{-4,7}
    \draw[fill=black] (p) circle (.2em) ++(0,\vNodeHeight) circle (.2em) ++(0,\vNodeHeight) circle (.2em);
    \vTranslate{-2,0}
    \vNode{88,89,90,91}
    \vOperations{+}
    \vNode{4,4,4,4}
    \vOperations{=}
    \vNode{92,93,94,95}
    \vArrowsStraight{J0/Mark12,J1/Mark13,J2/Mark14,J3/Mark15}
    \vTranslate{-6,4.5}
    \vNode{96,96}
    \vOperations{+}
    \vNode{0,1}
    \vOperations{=}
    \vNode{96,97}
    \vArrowsStraight{M0/Mark16,M1/Mark17}
    \vTranslate{-6,2.5}
    \vNode{98}
    \vOperations{+}
    \vNode{0}
    \vOperations{=}
    \vNode{98}
    \vArrowsStraight{P0/Mark18}
  \end{tikzpicture}
  \caption{Visualization of chunking the \code{iota} call with $\VSize{T}=4$ in \lst{simd foreach}.}
  \label{fig:simd iota}
\end{figure}

\begin{figure}[]
  \centering
  \begin{tikzpicture}
    \ttfamily
    \vInit
    \draw[fill=black] (p) circle (.2em) ++(0,\vNodeHeight) circle (.2em) ++(0,\vNodeHeight) circle (.2em);
    \vMemoryNode{5}
    \vMemoryMark[0.25]{2}{0,1,2,3,4,5,6}
    \vTranslate{-4,8}
    \vNode{92,93,94,95}
    \vOperations{*}
    \vNode{92,93,94,95}
    \vOperations{=}
    \vNode{92²,93²,94²,95²}
    \vTranslate{-6,4.5}
    \vNode{96,97}
    \vOperations{*}
    \vNode{96,97}
    \vOperations{=}
    \vNode{96²,97²}
    \vTranslate{-6,2.5}
    \vNode{98}
    \vOperations{*}
    \vNode{98}
    \vOperations{=}
    \vNode{98²}
    \draw[->] (Mark0.west) -- +(-2.0\vNodeWidth,0) |- (A0);
    \draw[->] (Mark1.west) -- +(-2.2\vNodeWidth,0) |- (A1);
    \draw[->] (Mark2.west) -- +(-2.4\vNodeWidth,0) |- (A2);
    \draw[->] (Mark3.west) -- +(-2.6\vNodeWidth,0) |- (A3);
    \draw[->] (Mark4.west) -- +(-2.8\vNodeWidth,0) |- (D0);
    \draw[->] (Mark5.west) -- +(-3.0\vNodeWidth,0) |- (D1);
    \draw[->] (Mark6.west) -- +(-3.2\vNodeWidth,0) |- (G0);
    \draw[<-] (Mark0.east) -- +(2.0\vNodeWidth,0) |- (C0);
    \draw[<-] (Mark1.east) -- +(2.2\vNodeWidth,0) |- (C1);
    \draw[<-] (Mark2.east) -- +(2.4\vNodeWidth,0) |- (C2);
    \draw[<-] (Mark3.east) -- +(2.6\vNodeWidth,0) |- (C3);
    \draw[<-] (Mark4.east) -- +(2.8\vNodeWidth,0) |- (F0);
    \draw[<-] (Mark5.east) -- +(3.0\vNodeWidth,0) |- (F1);
    \draw[<-] (Mark6.east) -- +(3.2\vNodeWidth,0) |- (I0);
  \end{tikzpicture}
  \caption{Visualization of chunking the \code{for_each} call with $\VSize{T}=4$ in \lst{simd foreach}.}
  \label{fig:simd foreach}
\end{figure}

\subsection{Discussion of algorithms}
\begin{description}
  \item[Copies]
    In general, the \simdEP policy requires algorithms to make a copy from the input sequence.
    For now, since \simd only supports arithmetic types and \simd does not return lvalue references to its values, it is not observable whether a copy was made.
    With two exceptions:
    \begin{itemize}
      \item Modification of the input sequence via different means than the function parameter(s), which is UB anyway, will not modify the value of the function parameter(s).
      \item Using mutable iterators, assignment to the \simd (lvalue reference) parameter of the user-supplied function object will not modify the output sequence until after the function has returned (cf. \lst{simd foreach implementation}).
    \end{itemize}
    Note that most non-modifying sequence operations allow modification of the sequence by using a non-const lvalue reference parameter for the user-supplied function object.

  \item[Predicates] Algorithms that take a predicate returning a \bool have two possible vectorization strategies:
    \begin{enumerate}
      \item The predicate still returns \bool.
        In this case, every predicate must execute a \code{simd_mask} reduction.
        This makes it simple to short-circuit in the algorithm implementation but may unnecessarily restrict the achievable parallelization.
      \item The predicate returns \code{simd_mask}.
        In this case \VSize{ForwardIterator::value\_type} reductions can happen in parallel.
        Short-circuiting is still possible, but requires a \code{simd_mask} reduction on each step (QoI question).
    \end{enumerate}
    It would also be possible to allow both and let the algorithm switch the strategy depending on the return type of the predicate.

    In Cologne 2019, LEWG unanimously recommended to exclusively go with predicates returning \code{simd_mask}:
    The use of \bool would effectively change the algorithms.

  \item[Complexity requirements]
    For many algorithms, the complexity requirement states “Applies \code{f} \emph{exactly} \code{last - first} times”.
    In the \simdEP case, the number of applications of \code{f} is reduced by an unspecified factor.

  \item[Sorting]
    The \code{Compare} function object type is required to return a value that is contextually convertible to \bool.
    For sorting, it is important that overloads using the \simdEP policy work with \code{simd_mask} instead of \bool.
    It is not useful for the sort algorithm to know whether all/any/some/none of the compared values are “less than”.
    It requires a mask object to know the “less than” relation for each individual value.
\end{description}

\subsection{Design Alternative}
In Cologne 2019, LEWG recommended to not pursue this design alternative.
It is still provided in this paper for completeness.

There are subtle differences in how the \simdEP specializations need to be used
(e.g. \code{std::generate} currently requires the generator function to return objects that can be assigned to a dereferenced \code{ForwardIt};
the \simdEP specialization requires the generator function to return objects of type \simd[<ForwardIt::value_type>]).
An attempt to fit \simdEPT into the existing wording results in some special-casing in the algorithm specifications.
This observation leads to the question whether a new execution policy is really the best approach.
The alternative would be a duplication of algorithms to variants with a \code{simd_} prefix in their name.
Example:
\smallskip\begin{lstlisting}[style=Vc]
simd_for_each(data.begin(), data.end(), [](auto &x) {
  x *= x;
});
\end{lstlisting}

This alternative would not reduce the amount of wording/complexity though, since now a lot of the algorithm wording would need to be duplicated.
However, this would allow a very simple reduction of the number of algorithms that support \simd execution.

\subsection{Affected algorithms}
The following algorithms have an \code{ExecutionPolicy} overload and can work with a \simdEPT specialization:
\begin{itemize}
  \item \code{all_of}, \code{any_of}, \code{none_of}
  \item \code{for_each}, \code{for_each_n}
  \item \code{find}, \code{find_if}, \code{find_if_not}
  \item \code{find_end}
  \item \code{find_first_of}
  \item \code{adjacent_find}
  \item \code{count}, \code{count_if}
  \item \code{mismatch}
  \item \code{equal}
  \item \code{search}, \code{search_n}
  \item \code{copy}, \code{copy_n} (no real need; can be implicitly vectorized)
  \item \code{copy_if}
  \item \code{swap} (no real need; can be implicitly vectorized)
  \item \code{transform}
  \item \code{replace}, \code{replace_if}, \code{replace_copy}, \code{replace_copy_if}
  \item \code{fill}, \code{fill_n} (no real need; can be implicitly vectorized)
  \item \code{generate}, \code{generate_n} (see \sect{generate})
  \item \code{remove}, \code{remove_copy} (no real need; can be implicitly vectorized)
  \item \code{remove_if}, \code{remove_copy_if}
  \item \code{unique}, \code{unique_copy}
  \item \code{reverse}, \code{reverse_copy} (no real need; can be implicitly vectorized)
  \item \code{rotate}, \code{rotate_copy} (no real need; can be implicitly vectorized)
  \item \code{is_partitioned}, \code{partition}, \code{stable_partition}, \code{partition_copy}, \code{partition_point}
  \item \code{sort},\code{stable_sort}, \code{partial_sort}, \code{partial_sort_copy}, \code{is_sorted}, \code{is_sorted_until}
  \item \code{nth_element}
  \item \code{merge}, \code{inplace_merge}
  \item \code{includes}, \code{set_union}, \code{set_intersection}, \code{set_difference}, \code{set_symmetric_difference}
  \item \code{min_element}, \code{max_element}, \code{minmax_element}
  \item \code{lexicographical_compare}
  \item \code{is_heap}, \code{is_heap_until}, \code{make_heap}, \code{push_heap}, \code{pop_heap}, and \code{sort_heap} (The comparison function object can use \code{simd} and \code{simd_mask}.)
\end{itemize}

The remaining algorithms have no obvious use for the specialization:
\begin{itemize}
  \item \code{move} makes no sense until we can create \code{simd<T>} types for pointers (likely) and class types (less likely).
\end{itemize}

\code{lower_bound}, \code{upper_bound}, \code{equal_range}, and \code{binary_search} may benefit from \simd usage, but currently do not provide \code{ExecutionPolicy} overloads.

\subsection{The \code{generate} algorithm}\label{sec:generate}
The generator function passed to \code{generate}/\code{generate_n} does not expect any arguments and thus has no interface for the algorithm to request a certain ABI tag from the function (template).
\begin{lstlisting}[style=Vc,numbers=left,float,label=lst:generate example,caption={
Use \code{std::generate} to fill a container with random numbers.
}]
std::vector<float> v;

// 0. Existing (scalar) interface
std::generate(v.begin(), v.end(), [&]() { return 0.f; })

// 1. Choose ABI via template parameter
// template <class Gen>
// void generate(/*...*/, Gen g) { g.template operator()<native_simd<float>>(); }
std::generate(v.begin(), v.end(), []<typename T>() { return T(); });

// 2. Choose ABI via return type
// template <class Gen>
// void generate(/*...*/, Gen g) {
//   auto x = g(); // discard if x has more values than needed
// }
std::generate(v.begin(), v.end(), []() { return native_simd<float>(); });

// 3. Choose ABI via unused parameter
// template <class Gen>
// void generate(/*...*/, Gen g) {
//   constexpr native_simd<float>* tag = nullptr;
//   g(tag);
// }
std::generate(v.begin(), v.end(), []<typename T>(T*) { return T(); });
\end{lstlisting}
Consequently, there are three ideas how to make it work for \code{simd}:
\begin{enumerate}
\item Require the generator function object to take a template argument (no function arguments).
\item Let the algorithm implementation cope with the return type defined by the generator function object.
  It may have to discard values if the number of values in the range is not a multiple of the number of values in the return type.
\item Pass a parameter that is not used other than for deducing the expected return type of the generator function object.
\end{enumerate}
See \lst{generate example} for examples.

Ideas 1 and 3 require the user to supply a type with a template call operator.
Idea 2 may lead the user to unintentionally drop generated values.
For the user, idea 3 requires to write boilerplate with no apparent use: it's an implementation detail shining through.
Idea 1 was preferred in the discussion in LEWG in Cologne 2019.
However, it is still unclear whether the need for explicit instantiation of the call operator in the standard library implementation is acceptable.
There is no apparent technical reason not to use this variant.

\subsection{Initial wording for the policy}

Add a new execution policy to \citep[§20.18.2]{N4842}:
\begin{wgText}[{§20.18.2 [execution.syn]}]
  \begingroup
  \ttfamily
  // 20.18.7, unsequenced execution policy\\
  class unsequenced_policy;\\
  \\
  \wgAdd{// 20.18.8, \simd{} execution policy}\\
  \wgAdd{class \simd{}_policy;}\\
  \\
  // 20.18.\wgRemove{8}\wgAdd{9}, execution policy objects:\\
  inline constexpr sequenced_policy seq\{ \textit{unspecified} \};\\
  inline constexpr parallel_policy par\{ \textit{unspecified} \};\\
  inline constexpr parallel_unsequenced_policy par_unseq\{ \textit{unspecified} \};\\
  inline constexpr unsequenced_policy unseq\{ \textit{unspecified} \};\\
  \wgAdd{inline constexpr \simd{}_policy \simd\{ \textit{unspecified} \};}
  \endgroup
\end{wgText}

Renumber §20.18.8 to §20.18.9 and add §20.18.8 [execpol.simd]:
\begin{wgText}
  \begin{itemdecl}
@\wgAdd{class \simd{}_policy \{ \textit{unspecified} \};}@
  \end{itemdecl}
  \begin{itemdescr}
    \pnum\wgAdd{The class \type{simd_policy} is an execution policy type used as a unique type to disambiguate parallel algorithm overloading and indicate that a parallel algorithm's execution may be vectorized using \simd{} for interfacing with user-provided functionality.}

    \pnum\wgAdd{During the execution of a parallel algorithm with the \simdEPT{} policy, if the invocation of an element access function exits via an uncaught exception, \code{terminate()} shall be called.}
  \end{itemdescr}
\end{wgText}

Add to §20.18.9 [execpol.objects]:
\begin{itemdecl}
@\wgAdd{inline constexpr \simdEPT{} \simdEP{}\{ \textit{unspecified} \};}@
\end{itemdecl}

\bigskip
\citep[§25.3.2]{N4842} defines requirements on user-provided function objects.
This might be the right place to add:
\begin{wgText}[{§25.3.2 [algorithms.parallel.user]}]
  \setcounter{Paras}{1}
  \color{WgAdd}
  \pnum Function objects passed into parallel algorithms instantiated with the \simdEP execution policy shall:
  \begin{itemize}
    \item be callable with arguments of type \simd[<Iterator::value_type, Abi>], for any ABI tag \code{Abi}, for all arguments that otherwise would be of type \code{Iterator::value_type};
    \item return objects of type \simd[<Iterator::value_type, Abi>], if the function object is otherwise expected to return objects assignable to a dereferenced \code{Iterator} object;
    \item return objects of type \mask[<Iterator::value_type, Abi>] or \bool, if the function object is otherwise expected to return \bool.
  \end{itemize}
\end{wgText}

The following subsection in \citep[§25.3.3]{N4842} defines the semantics of the execution policies.
A new paragraph for \simdEP is needed.
The intent is to
\begin{enumerate}
  \item constrain execution to the calling thread,
  \item allow implementations to assume unordered access for all internal element access functions (most importantly loads and stores),
  \item apply user-provided function objects in the order the \simd chunks are created from sequential iteration over the iterator(s).
\end{enumerate}
\begin{wgText}[{§25.3.3 [algorithms.parallel.exec]}]
  \setcounter{Paras}{7}%
  \color{WgAdd}
  \pnum
  The invocations of element access functions in parallel algorithms invoked with an execution policy object of type \simdEPT are permitted to execute in an unordered fashion in the calling thread, except for the application of user-provided function objects.
  User-provided function objects are called with an unspecified number of sequence elements combined into a \simd[<T, Abi>] object.
  The type for \type{Abi} is chosen by the implementation.
  It may be different for subsequent applications of the user-provided function in the same parallel algorithm invocation.
  The type for \type T is the decayed type of the sequence elements.
  The order of elements in the \simd object is equal to the order of the corresponding elements in the sequence argument.
  The invocation order of user-provided function objects is sequential.
\end{wgText}

\newcommand\tmp{[algorithms.parallel.exceptions]}
It is my understanding that we do not want to add anything to \citep[§25.3.4 \tmp{}]{N4842} at this point.
The situation is simpler for the \simdEP policy.
It is almost equivalent to the \code{seq} policy.

\subsection{Wording for individual algorithms}
\begin{wgText}[{§25.7 [alg.sorting]}]
  \setcounter{Paras}{1}%
    \pnum
    \code{Compare} is a function object type.
    The return value of the function call operation applied to an object of type \code{Compare}, when contextually converted to \bool, yields \true if the first argument of the call is less than the second, and \false otherwise.
    \wgAdd{
      If the \code{ExecutionPolicy} is \simdEPT, the return type of the function call operation applied to an object of type \code{Compare} is a specialization of \mask.
      Its $i$-th element in the \mask{} yields \true{} if the value of the $i$-th element of the first argument of the call is less than the corresponding element of the second, and \false{} otherwise.
    }
    \code{Compare comp} is used throughout for algorithms assuming an ordering relation.
    It is assumed that \code{comp} will not apply any non-constant function through the dereferenced iterator.

\end{wgText}

Further wording work is necessary where individual algorithms refer to boolean results from predicates. (E.g. \code{all_of} returns \code{false} if \code{any_of(}$E$\code{)} is \false\ldots)

% vim: sw=2 et ft=tex spell tw=0
