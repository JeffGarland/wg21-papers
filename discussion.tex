\section{Discussion}

\subsection{Member Types}
The member types may not seem obvious.
Rationales:
\begin{typelist*}
  \item[value_type]
    In the spirit of the \valuetype member of STL containers, this type denotes the \emph{logical} type of the values in the vector.

  \item[register_value_type]
    On some targets it may be beneficial to implement \datapar instantiations of some \type T with a different type \type{register_value_type}, which has higher precision than \type T.
    This is mostly an implementation detail, but can be important to know in some situations, especially whenever \type{native_handle_type} is involved.

    \guidance{A better name might be \type{native_value_type}.}

  \item[native_handle_type]
    The type used for enabling access to an implementation-defined member object (via the \code{native_handle()} function).

  \item[reference]
    Used as the return type of the non-const scalar subscript operator.
    This may use implementation-defined means to solve possible type aliasing issues.

  \item[const_reference]
    Used as the return type of the const scalar subscript operator.
    From my experience with Vc, it is safest to actually not use a const lvalue reference here, but a temporary.

  \item[mask_type]
    The natural mask type for this \datapar instantiation.
    This type is used as return type of compares and write-mask on assignments.

  \item[size_type]
    Standard member type used for \code{size()} and \code{operator[]}.

  \item[abi_type]
    The \type{Abi} template parameter to \datapar.

\end{typelist*}

\subsection{Default Construction}
The default constructors of \datapar and \mask zero-initialize the object.
This is important for compatibility with \code{\type T()}, which zero-initializes fundamental types.
There may be a concern that unnecessary initialization could lead to unnecessary instructions.
I consider this a QoI issue.
Implementations are certainly able to recognize unnecessary initializations in many cases.

\subsection{Conversions}
The \datapar conversion constructor only allows implicit conversion from \datapar template instantiations with the same \type{Abi} type and compatible \valuetype.
Discussion in SG1 showed clear preference for only allowing implicit conversion between integral types that only differ in signedness.
All other conversions could be implemented via an explicit conversion constructor.
The alternative (preferred) is to use \simdcast consistently for all other conversions.

\subsection{Broadcast Constructor}
The broadcast constructor is not declared as \code{explicit} to ease the use of scalar prvalues in expressions involving data-parallel operations.
The operations where such a conversion should not be implicit consequently need to use SFINAE / concepts to inhibit the conversion.

\subsection{Aliasing of Subscript Operators}
Note that the way the subscript operators are declared, some kind of type punning needs to happen.\footnote{
  Note: The vector builtins of clang do not suffice to implement the subscript operators, even though they support subscripting the vector object.
  An implementation might have to use a mechanism such as the \code{gnu::may_alias} attribute.
}
An lvalue reference to \type{register_value_type} needs to reference the same object as is contained as one element of the \datapar object.
An alternative to an lvalue reference would be a smart reference object.
This would require progress on language improvements for smart references first.

The subscript operator of the \mask type, on the other hand, may not use lvalue references and must use a smart reference wrapper instead.
This is necessary because there are systems where a single boolean element is stored as a single bit.
To ensure source compatibility the return type must therefore be a smart reference on all implementations.

\subsection{Parameters of Binary and Compare Operators}
It is easier to implement and possibly easier to specify these operators if the signature is specified as:
\begin{itemdecl}
template <class T, class U>
datapar_return_type<T, U> operator+(const T &, const U &);
\end{itemdecl}
The motivation for using a variant where at least one function parameter is constrained to the \datapar class template is compilation speed.
The compiler can drop the operator from the overload resolution set quicker.
With concepts it might be worthwhile to revisit this decision.

\subsection{Compound Assignment}
The semantics of compound assignment would allow less strict implicit conversion rules.
Consider \code{datapar<int>() *= double()}: the corresponding multiplication operator would not compile because the implicit conversion to \datapar[<float>] is non-portable.
Compound assignment, on the other hand, implies an implicit conversion back to the type of the expression on the left of the assignment operator.
Thus, it is possible to define compound operators that execute the operation correctly on the promoted type without sacrificing portability.
There are two arguments for not relaxing the rules for compound assignment, though:
\begin{enumerate}
  \item Consistency: The conversion of an expression with compound assignment to a binary operator suddenly would not compile anymore.
  \item The implicit conversion in the \code{int * double} case could be expensive and unintended.
    This is already a problem for builtin types where many developers multiply \float variables with \double prvalues.
\end{enumerate}

\subsection{Return Type of Masked Assignment Operators}
The assignment operators of the type returned by \code{where(mask, datapar)} could return one of:
\begin{itemize}
  \item A reference to the \datapar object that was modified.
  \item A temporary \datapar object that only contains the elements where the \mask is \true.
  \item The object returned from the \code{where} function.
  \item Nothing (\ie \void).
\end{itemize}
My first choice was a reference to the modified \datapar object.
However, then the statement \code{(where(x < 0, x) *= -1) += 2} may be surprising: it adds \code 2 to all vector entries, independent of the mask.
Likewise, \code{y += (where(x < 0, x) *= -1)} has a possibly confusing interpretation because of the \mask in the middle of the expression.

Consider that write-masked assignment is used as a replacement for \code{if}-statements.
Using \void as return type therefore is a more fitting choice because \code{if}-statements have no return value.
By declaring the return type as \void the above expressions become ill-formed, which seems to be the best solution for guiding users to write maintainable code and express intent clearly.

\subsection{Fundamental SIMD Type or Not?}
\subsubsection{The Issue}
There has been renewed discussion on the reflectors over the question whether \CC{} should define a fundamental, native SIMD type (let us call it \type{fundamental<T>}) and a generic data-parallel type on top which supports an arbitrary number of elements (call it \type{arbitrary<T, N>}).
The alternative to defining both types is to only define \type{arbitrary<T, N = default_size<T>>}, since it encompasses the \type{fundamental<T>} type.

With regard to this proposal this second approach would add a third template parameter to \datapar and \mask as shown in \lst{datapar N}.
\begin{lstlisting}[style=Vc,numbers=left,float,label=lst:datapar N,caption={
  Possible declaration of the class template parameters of a \datapar class with arbitrary width.
}]
template <class T, size_t N = datapar_size_v<T, datapar_abi::compatible>,
          class Abi = datapar_abi::compatible>
class datapar;
\end{lstlisting}

\subsubsection{Standpoints}
The controversy is about how the flexibility of a type with arbitrary \code N is presented to the users.
Is there a (clear) distinction between a “fundamental” type with target-dependent (i.e. fixed) \code N and a higher-level abstraction with arbitrary \code N which can potentially compile to inefficient machine code.
Or should the \CC{} standard only define \type{arbitrary} and set it to a default \code N value that corresponds to the target-dependent \code N.
Thus, the default \code N, of \type{arbitrary} would correspond to \type{fundamental}.

It is interesting to note that \type{arbitrary<T, 1>} is the class variant of \type T.
Consequently, if we say there is no need for a \type{fundamental} type then we could argue for the deprecation of the builtin arithmetic types, in favor of \type{arbitrary<T, 1>}. \wgNote{This is an academic discussion, of course.}

The author has implemented a library where a clear distinction is made between \type{fundamental<T, Abi>} and \type{arbitrary<T, N>}.
The documentation and all teaching material says that the user should program with \type{fundamental}.
The \type{arbitrary} type should be used in special circumstances, or wherever \type{fundamental} works with the \type{arbitrary} type in its interfaces (e.g. for gather \& scatter or the \code{ldexp} \& \code{frexp} functions).

\subsubsection{Issues}
The definition of two separate class templates can alleviate some source compatibility issues resulting from different \code N on different target systems.
Consider the simplest example of a multiplication of an \intt vector with a \float vector:
\smallskip\begin{lstlisting}[style=Vc]
arbitrary<float>() * arbitrary<int>();  // compiles for some targets, fails for others
fundamental<float>() * fundamental<int>();  // never compiles, requires explicit cast
\end{lstlisting}
The \datapar[<T>] operators are specified in such a way that source compatibility is ensured.
For a type with user definable \code N, the binary operators should work slightly different with regard to implicit conversions.
Most importantly, \type{arbitrary<T, N>} solves the issue of portable code containing mixed integral and floating-point values.
A user would typically create aliases such as:
\smallskip\begin{lstlisting}[style=Vc]
using floatvec = datapar<float>;
using intvec = arbitrary<int, floatvec::size()>;
using doublevec = arbitrary<int, floatvec::size()>;
\end{lstlisting}
Objects of types \type{floatvec}, \type{intvec}, and \type{doublevec} will work together independent of the target system.

Obviously, these type aliases are basically the same if the \code N parameter of \type{arbitrary} has a default value:
\smallskip\begin{lstlisting}[style=Vc]
using floatvec = arbitrary<float>;
using intvec = arbitrary<int, floatvec::size()>;
using doublevec = arbitrary<int, floatvec::size()>;
\end{lstlisting}
The ability to create these aliases is not the issue.
Seeing the need for using such a pattern is the issue.
Typically, a developer will think no more of it if his code compiles on his machine.
If \code{arbitrary<float>() * arbitrary<int>()} just happens to compile (which is likely) then this is the code that will get checked in to the repository.
Note that with the existence of the \type{fundamental} class template, the \code N parameter of the \type{arbitrary} class would not have a default value and thus force the user to think a second longer about portability.

%Consider the \code{ldexp} function in \lst{ldexp}.
%\begin{lstlisting}[style=Vc,numbers=left,float,label=lst:ldexp,caption={
%  Declaration of \code{ldexp} for \type{fundamental} and \type{arbitrary} (only single-precision \float).
%}]
%template <class Abi>
%fundamental<float, Abi> ldexp(fundamental<float, Abi> x,
%                              arbitrary<int, fundamental<float, Abi>::size()> exp);
%template <int N>
%arbitrary<float, N> ldexp(arbitrary<float, N> x, arbitrary<int, N> exp);
%\end{lstlisting}
%%\lst{fundamental-ldexp}
%\begin{lstlisting}[style=Vc,numbers=left,float,label=lst:fundamental-ldexp,caption={
%  Calls to \code{ldexp} compile or fail to compile independent of the target.
%}]
%fundamental<float> a = ...;
%fundamental<int> exp = ...;
%a = ldexp(a, exp);  // compiles nowhere
%\end{lstlisting}
%%\lst{arbitrary-ldexp}
%\begin{lstlisting}[style=Vc,numbers=left,float,label=lst:arbitrary-ldexp,caption={
%  Calls to \code{ldexp} compile or fail to compile depending on the target.
%}]
%arbitrary<float> a = ...;
%arbitrary<int> exp = ...;
%a = ldexp(a, exp);  // compiles where a.size() == exp.size(), fails otherwise
%\end{lstlisting}
%
%
%The \type{fundamental} and \type{arbitrary} types have mostly the same interface.
%It is interesting to make \type{arbitrary} behave slightly differently, catering to the special use cases where \type{fundamental} is designed for safety.
%Consider implicit conversions (cf. \lst{implicit conversions}): \type{fundamental} must be very restrictive with regard to implicit conversions.
%\begin{lstlisting}[style=Vc,numbers=left,float,label=lst:implicit conversion,caption={
%  Implicit conversions behave differently for \type{fundamental} and \type{arbitrary}.
%}]
%auto x0 = fundamental<int>() * 1.f; // fails to compile since fundamental<int>::size()
%                                    // might be different to fundamental<float>::size()
%auto x1 = arbitrary<int>() * 1.f;   // x1 is of type arbitrary<float,
%                                    // arbitrary<int>::size()>
%\end{lstlisting}
%Implicit conversions are only allowed where \code{size()} is equal for the two types for each conceivable target systems.\footnote{
%  The safest approach is to disallow all implicit conversions.
%  Vc allows conversion between signed and unsigned integral types.
%}
%The \type{arbitrary} type, on the other hand, can allow implicit conversions to work much more alike to the fundamental arithmetic types.
