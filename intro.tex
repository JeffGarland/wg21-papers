\section{Introduction}
(Contains minor improvements and clarifications compared to the earlier revisions.)

\subsection{SIMD Registers and Operations}
Since many years the number of SIMD instructions and the size of SIMD registers have been growing.
Newer microarchitectures introduce new operations for optimizing certain (common or specialized) operations.
Additionally, the size of SIMD registers has increased and may increase further in the future.

The typical minimal set of SIMD instructions for a given scalar data type comes down to the following:
\begin{itemize}
  \item Load instructions: load \VSize{T} successive scalar values starting from a given address into a SIMD register.
  \item Store instructions: store from a SIMD register to \VSize{T} successive scalar values at a given address.
  \item Arithmetic instructions: apply the arithmetic operation to each pair of scalar values in the two SIMD registers and store the results back to a SIMD register.
  \item Compare instructions: apply the compare operation to each pair of scalar values in the two SIMD registers and store the results back to a SIMD mask register.
  \item Bitwise instructions: bitwise operations on SIMD registers.
  \item Shuffle instructions: permutation and/or blending of scalars in (a) SIMD register(s).
\end{itemize}

The set of available instructions may differ considerably between different microarchitectures of the same CPU family.
Furthermore there are different SIMD register sizes.
Future extensions will certainly add more instructions and larger SIMD registers.

\subsection{Motivation for Data-Parallel Types}
SIMD registers and operations are the low-level ingredients to efficient programming for SIMD CPUs.
At a more abstract level this is is not only about SIMD CPUs, but efficient data-parallel execution (CPUs, GPUs, possibly FPGAs and classical vector supercomputers).
Operations on fundamental types in \CC{} form the abstraction for CPU registers and instructions.
Thus, a data-parallel type (SIMD type) can provide the necessary interface for writing software that can utilize data-parallel hardware efficiently.
Higher-level abstractions can be built on top of these types.
Note that if a low-level access to SIMD is not provided, users of \CC{} are either constrained to work within the limits of the provided abstraction or resort to non-portable extensions, such as SIMD intrinsics.

In some cases the compiler might generate better code if only the intent is stated instead of an exact sequence of operations.
Therefore, higher-level abstractions might seem preferable to low-level SIMD types.
In my experience this is a non-issue because programming with SIMD types makes intent very clear and compilers can optimize sequences of SIMD operations just like they can for scalar operations.
SIMD types do not lead to an easy and obvious answer for efficient and easily usable data structures, though.
But, in contrast to vector loops, SIMD types make unsuitable data structures glaringly obvious and can significantly support the developer in creating more suitable data layouts.

One major benefit from SIMD types is that the programmer can gain an intuition for SIMD.
This subsequently influences further design of data structures and algorithms to better suit SIMD architectures.

There are already many users of SIMD intrinsics (and thus a primitive form of SIMD types).
Providing a cleaner and portable SIMD API would provide many of them with a better alternative.
Thus, SIMD types in \CC{} would capture and improve on widespread existing practice.

The challenge remains in providing \emph{portable} SIMD types and operations.

\subsection{Problem}
\CC{} has no means to use SIMD operations directly.
There are indirect uses through automatic loop vectorization or optimized algorithms (that use extensions to C/\CC{} or assembly for their implementation).

All compiler vendors (that I worked with) add intrinsics support to their compiler products to make SIMD operations accessible from C.
These intrinsics are inherently not portable and most of the time very directly bound to a specific instruction.
(Compilers are able to statically evaluate and optimize SIMD code written via intrinsics, though.)

% ft=tex tw=0 et sw=2 spell
